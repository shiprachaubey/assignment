# BlinkIt Category & Subcategory Scraper

## Overview

This project contains a Python-based scraper for extracting BlinkIt product data by category and location, specifically designed for the Dcluttr Software Developer recruitment Task 1. The scraper utilizes BlinkIt’s **public API endpoint** to fetch product listings from given categories and geographical locations (latitude and longitude).

The script handles:
- Dynamic requests with headers and cookies imitating a real browser session using the `curl_cffi` HTTP client.
- Pagination and retries with basic rate-limit handling.
- Extraction of relevant product fields according to a predefined schema.
- Export of collected data to a CSV file for further analysis.

## Features

- Reads input locations and categories from CSV files.
- Sends POST requests to BlinkIt’s `/v1/layout/listing_widgets` API.
- Extracts product-level details including pricing, stock, brand, and sponsorship status.
- Manages connection retries on rate-limit (HTTP 429).
- Polite scraping with randomized delays.
- Output aligned with a provided schema file.

## File Structure

| File                           | Description                                        |
|--------------------------------|--------------------------------------------------|
| `main.py`                      | Main scraping script (renamed to `perform_blinkit_scraping_v2` function) |
| `blinkit_locations.csv`        | Input CSV with latitude and longitude columns for scraping target locations |
| `blinkit_categories.csv`       | Input CSV defining categories and subcategories to scrape |
| `Scraping Task _ Schema - Schema.csv` | CSV defining the output schema columns (first row skipped in code) |
| `blinkit_scraped_data_final.csv` | Output CSV generated by the script with scraped product data |
| `requirements.txt`             | Required Python packages (`pandas`, `curl-cffi`) |

## Setup

1. **Clone this repository** (or place the files in a working directory).

2. **Prepare the environment**  
   The script requires Python 3.8+.

3. **Install dependencies**

   Using pip:

   ```
   pip install -r requirements.txt
   
   ```

   Or using conda (recommended):
   
   ```
    conda create -n blinkit_scraper python=3.10 pandas
    conda activate blinkit_scraper
    pip install curl-cffi

    ```

4. **Prepare Input Files**

- `blinkit_locations.csv`: Must include columns `latitude` and `longitude`.
- `blinkit_categories.csv`: Must include columns `l1_category`, `l1_category_id`, `l2_category`, and `l2_category_id`.
- `Scraping Task _ Schema - Schema.csv`: Defines the output CSV columns.

## Usage

Run the scraper script via command line:

   ```
   python main.py
   ```

The script will:
- Iterate over each location and category/subcategory combination.
- Perform API requests with proper headers and cookies.
- Store all gathered product data.
- Save the final dataset to `blinkit_scraped_data_final.csv` in the same directory.

## Important Notes

### Headers & Authentication

- The script uses a hardcoded `auth_key` and other request headers which **must be updated frequently** for successful scraping.
- You must inspect a valid browser session (Chrome DevTools → Network) and copy fresh values for:
  - `auth_key`
  - `user-agent`
  - Cookies like device ID and location data
- Failure to update these headers will lead to HTTP 403 or connection resets from the BlinkIt API.

### Rate Limiting

- The script implements basic retry logic and waits if HTTP 429 response is encountered.
- Do not run rapid-fire scraping to avoid bans.
- Sleep times are randomized between requests for politeness.

### Data Schema

- Output columns are matched to the schema CSV file (skipping the header row).
- If schema changes, update the `Scraping Task _ Schema - Schema.csv` accordingly.


